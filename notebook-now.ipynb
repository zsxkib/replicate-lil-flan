{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    LogitsWarper,\n",
    "    LogitsProcessor,\n",
    "    LogitsProcessorList,\n",
    ")\n",
    "from cog import BasePredictor, Input\n",
    "from typing import Dict\n",
    "import torch\n",
    "\n",
    "CACHE_DIR = \"./src/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\", cache_dir=CACHE_DIR)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\", cache_dir=CACHE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Hello, my name is \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[11859,     1]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# token for \"Greg\"\n",
    "tokenizer(\"Greg\", return_tensors=\"pt\").input_ids  # tensor([[11859,     1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:  tensor([[8774,    6,   82,  564,   19,    3,    1]])\n",
      "outputs:  tensor([[  0,   3,  23, 265,   3,   7, 265,  76,  15,  40,   3,   7,   9, 967,\n",
      "           1]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'iam samuel sailor'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "print(\"inputs: \", inputs)\n",
    "outputs = model.generate(inputs)\n",
    "print(\"outputs: \", outputs)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasLogitsWarper(LogitsWarper):\n",
    "    \"\"\"Applies a bias to the logits of specific tokens before softmax.\n",
    "\n",
    "    This class can be used with the `LogitsProcessorList` in Hugging Face's Transformers\n",
    "    library to alter the logits produced by a model before softmax is applied during\n",
    "    text generation.\n",
    "\n",
    "    The class is not dependent on the `input_ids` as it applies bias to specific token ids\n",
    "    regardless of the context or sequence of tokens currently being processed.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    bias : Dict[int, float]\n",
    "        A dictionary mapping from token ids to bias values. The bias is added to the\n",
    "        logits for the corresponding token id. If the bias is -100 or 100, the logit for\n",
    "        the token id is set to negative or positive infinity, respectively, to essentially\n",
    "        ban or guarantee the token.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __call__(input_ids: torch.LongTensor, scores: torch.FloatTensor)\n",
    "        Applies the bias to the logits. This method is called during the generation process.\n",
    "\n",
    "    warp_logits(logits: torch.Tensor) -> torch.Tensor\n",
    "        The method that actually applies the bias to the logits.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    bias_dict = {11859: 8}  # We're using 8 here to heavily bias towards \"Greg\" (token 11859)\n",
    "    bias_warper = BiasLogitsWarper(bias_dict)\n",
    "    logits_processor_list = LogitsProcessorList([bias_warper])\n",
    "    outputs = model.generate(inputs, logits_processor=logits_processor_list)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bias: Dict[int, float]):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        bias : Dict[int, float]\n",
    "            A dictionary mapping from token ids to bias values.\n",
    "        \"\"\"\n",
    "        self.bias = bias\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        \"\"\"The method called during the generation process.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_ids : torch.LongTensor\n",
    "            The input ids for the current generation step. Not used in this class.\n",
    "        scores : torch.FloatTensor\n",
    "            The logits for the current generation step.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The modified logits.\n",
    "        \"\"\"\n",
    "        return self.warp_logits(scores)\n",
    "\n",
    "    def warp_logits(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Applies the bias to the logits.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        logits : torch.Tensor\n",
    "            The logits for the current generation step.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The modified logits.\n",
    "        \"\"\"\n",
    "        for token_id, bias_value in self.bias.items():\n",
    "            if abs(bias_value) == 100:\n",
    "                # Set logit to extremely high or low value\n",
    "                new_logit = float(\"inf\") if bias_value > 0 else -float(\"inf\")\n",
    "                logits[..., token_id] = new_logit\n",
    "            else:\n",
    "                # Add bias to logit\n",
    "                logits[..., token_id] += bias_value\n",
    "\n",
    "            new_logit = logits[..., token_id].item()\n",
    "            # print(f\"New logit for token {token_id}: {new_logit}\")\n",
    "\n",
    "        # The '...' (ellipsis) is used here to index into any number of dimensions,\n",
    "        # For example, if logits is a 3D tensor with shape (batch_size, sequence_length, vocab_size),\n",
    "        # logits[..., token_id] would be equivalent to logits[:, :, token_id].\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_tokens_to_bias(tokenizer, bias_dict_str: Dict[str, float]) -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    Maps a dictionary with strings as keys to a dictionary with corresponding token IDs as keys.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokenizer\n",
    "        The tokenizer to use for translating the strings into token IDs.\n",
    "    bias_dict_str : Dict[str, float]\n",
    "        A dictionary mapping from strings to bias values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[int, float]\n",
    "        The resulting dictionary mapping from token IDs to bias values.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> bias_dict_str = {\"Greg\": 8, \"Sam\": -10}\n",
    "    >>> bias_dict = map_tokens_to_bias(tokenizer, bias_dict_str) # {11859: 8, 3084: -10}\n",
    "    \"\"\"\n",
    "    return {\n",
    "        tokenizer.encode(token, add_special_tokens=False)[0]: bias for token, bias in bias_dict_str.items()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greg Gregg\n"
     ]
    }
   ],
   "source": [
    "bias_dict_str = {\"Greg\": 8, \"Sam\": -10}  # The user wants to increase the likelihood of \"Greg\"\n",
    "bias_dict = map_tokens_to_bias(tokenizer, bias_dict_str)\n",
    "\n",
    "bias_warper = BiasLogitsWarper(bias_dict)\n",
    "logits_processor_list = LogitsProcessorList([bias_warper])\n",
    "\n",
    "outputs = model.generate(inputs, logits_processor=logits_processor_list)\n",
    "\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New logit for token 11859: 5.309817790985107\n",
      "New logit for token 11859: 5.580060005187988\n",
      "New logit for token 11859: 2.9869871139526367\n",
      "New logit for token 11859: 2.290940284729004\n",
      "Greg Gregg\n"
     ]
    }
   ],
   "source": [
    "bias_dict = {11859: 8}  # We're using 8 here to heavily bias towards \"Greg\"\n",
    "bias_warper = BiasLogitsWarper(bias_dict)\n",
    "logits_processor_list = LogitsProcessorList([bias_warper])\n",
    "\n",
    "# Using our custom LogitsProcessorList with the bias_warper in the call to generate\n",
    "outputs = model.generate(inputs, logits_processor=logits_processor_list)\n",
    "\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.309817790985107"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-2.6901822090148926 + 8.0\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Greg': 8, 'Sam': 10}\n",
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam Sam'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    LogitsWarper,\n",
    "    LogitsProcessorList,\n",
    ")\n",
    "from cog import BasePredictor, Input\n",
    "from typing import Dict\n",
    "import torch\n",
    "import json5\n",
    "import os\n",
    "\n",
    "CACHE_DIR = \"./src/models\"\n",
    "\n",
    "\n",
    "class BiasLogitsWarper(LogitsWarper):\n",
    "    \"\"\"Applies a bias to the logits of specific tokens before softmax.\n",
    "\n",
    "    This class can be used with the `LogitsProcessorList` in Hugging Face's Transformers\n",
    "    library to alter the logits produced by a model before softmax is applied during\n",
    "    text generation.\n",
    "\n",
    "    The class is not dependent on the `input_ids` as it applies bias to specific token ids\n",
    "    regardless of the context or sequence of tokens currently being processed.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    bias : Dict[int, float]\n",
    "        A dictionary mapping from token ids to bias values. The bias is added to the\n",
    "        logits for the corresponding token id. If the bias is -100 or 100, the logit for\n",
    "        the token id is set to negative or positive infinity, respectively, to essentially\n",
    "        ban or guarantee the token.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __call__(input_ids: torch.LongTensor, scores: torch.FloatTensor)\n",
    "        Applies the bias to the logits. This method is called during the generation process.\n",
    "\n",
    "    warp_logits(logits: torch.Tensor) -> torch.Tensor\n",
    "        The method that actually applies the bias to the logits.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    bias_dict = {11859: 8}  # We're using 8 here to heavily bias towards \"Greg\" (token 11859)\n",
    "    bias_warper = BiasLogitsWarper(bias_dict)\n",
    "    logits_processor_list = LogitsProcessorList([bias_warper])\n",
    "    outputs = model.generate(inputs, logits_processor=logits_processor_list)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bias: Dict[int, float]):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        bias : Dict[int, float]\n",
    "            A dictionary mapping from token ids to bias values.\n",
    "        \"\"\"\n",
    "\n",
    "        self.bias = bias\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        \"\"\"The method called during the generation process.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_ids : torch.LongTensor\n",
    "            The input ids for the current generation step. Not used in this class.\n",
    "        scores : torch.FloatTensor\n",
    "            The logits for the current generation step.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The modified logits.\n",
    "        \"\"\"\n",
    "\n",
    "        # input_ids not used because biases are applied to scores (logits) over the entire vocab\n",
    "        # So, we don't really need input_ids, it's just a formality outlined by the LogitsWarper ABC\n",
    "        return self.warp_logits(scores)\n",
    "\n",
    "    def warp_logits(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Applies the bias to the logits.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        logits : torch.Tensor\n",
    "            The logits for the current generation step.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The modified logits.\n",
    "        \"\"\"\n",
    "\n",
    "        for token_id, bias_value in self.bias.items():\n",
    "            if abs(bias_value) == 100:\n",
    "                # Set logit to extremely high or low value\n",
    "                new_logit = float(\"inf\") if bias_value > 0 else -float(\"inf\")\n",
    "                logits[..., token_id] = new_logit\n",
    "            else:\n",
    "                # Add bias to logit\n",
    "                logits[..., token_id] += bias_value\n",
    "            new_logit = logits[..., token_id].item()\n",
    "            # print(f\"New logit for token {token_id}: {new_logit}\")\n",
    "            \n",
    "        # The '...' (ellipsis) is used here to index into any number of dimensions,\n",
    "        # For example, if logits is a 3D tensor with shape (batch_size, sequence_length, vocab_size),\n",
    "        # logits[..., token_id] would be equivalent to logits[:, :, token_id].\n",
    "        return logits\n",
    "\n",
    "\n",
    "class Predictor(BasePredictor):\n",
    "    def setup(self):\n",
    "        \"\"\"Load the model into memory to make running multiple predictions efficient\"\"\"\n",
    "\n",
    "        os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\", cache_dir=CACHE_DIR)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\", cache_dir=CACHE_DIR)\n",
    "\n",
    "    def map_tokens_to_bias(self, bias_dict_str: Dict[str, float]) -> Dict[int, float]:\n",
    "        \"\"\"\n",
    "        Maps a dictionary with strings as keys to a dictionary with corresponding token IDs as keys.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tokenizer\n",
    "            The tokenizer to use for translating the strings into token IDs.\n",
    "        bias_dict_str : Dict[str, float]\n",
    "            A dictionary mapping from strings to bias values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dict[int, float]\n",
    "            The resulting dictionary mapping from token IDs to bias values.\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        >>> bias_dict_str = {\"Greg\": 8, \"Sam\": -10}\n",
    "        >>> bias_dict = map_tokens_to_bias(bias_dict_str) # {11859: 8, 3084: -10}\n",
    "        \"\"\"\n",
    "\n",
    "        bias_dict = {}\n",
    "        for token, bias in bias_dict_str.items():\n",
    "            token_ids = self.tokenizer.encode(token, add_special_tokens=False)\n",
    "            if len(token_ids) > 1:\n",
    "                raise ValueError(f\"The string '{token}' corresponds to more than one token in the tokenizer.\")\n",
    "            bias_dict[token_ids[0]] = bias\n",
    "        return bias_dict\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        prompt: str = Input(description=\"Prompt for language model\"),\n",
    "        bias_dict_str: str = Input(description=\"Dictionary mapping from strings to bias values\", default=None),\n",
    "        max_output_len: int = Input(description=\"Maximum length of output\", default=64, ge=1, le=512),\n",
    "    ) -> str:\n",
    "        \"\"\"Run a single prediction on the model\"\"\"\n",
    "\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "        logits_processor_list = None\n",
    "        if bias_dict_str is not None:\n",
    "            word_to_bias_dict = json5.loads(bias_dict_str)\n",
    "            tokenid_to_bias_dict = self.map_tokens_to_bias(word_to_bias_dict)\n",
    "            bias_warper = BiasLogitsWarper(tokenid_to_bias_dict)\n",
    "            logits_processor_list = LogitsProcessorList([bias_warper])\n",
    "        outputs = self.model.generate(\n",
    "            inputs,\n",
    "            max_new_tokens=max_output_len,\n",
    "            logits_processor=logits_processor_list,\n",
    "        )\n",
    "        return self.tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "\n",
    "p = Predictor()\n",
    "p.setup()\n",
    "p.predict(\"Hello, my name is\", \"{'Greg': 8, 'Sam': 10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement json4 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for json4\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install json4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All test cases passed.\n"
     ]
    }
   ],
   "source": [
    "import json5\n",
    "\n",
    "\n",
    "def str_to_dict(s):\n",
    "    s = s.replace(\"'\", '\"').strip()\n",
    "    return json5.loads(s)\n",
    "\n",
    "\n",
    "bias_dict_str = \"{'Greg': 8, 'Sam': -10}\"\n",
    "bias_dict = str_to_dict(bias_dict_str)\n",
    "\n",
    "\n",
    "bias_dict_str = \"{'Greg': 8, 'Sam': -10}\"\n",
    "\n",
    "# {\"Greg\": 8, \"Sam\": -10}  -> {\"Greg\": 8, \"Sam\": -10}\n",
    "# \"{'Greg': 8, 'Sam': -10}\" -> {\"Greg\": 8, \"Sam\": -10}\n",
    "# \"\"\"{'Greg': 8, 'Sam': -10}\"\"\" -> {\"Greg\": 8, \"Sam\": -10}\n",
    "# \"\"\"{\"Greg\": 8, \"Sam\": -10}\"\"\" -> {\"Greg\": 8, \"Sam\": -10}\n",
    "# \"\"\" {\"Greg\":  8, \"Sam\" : -10 }\"\"\" -> {\"Greg\": 8, \"Sam\": -10}\n",
    "# '{\"Greg\": 8, \"Sam\": -10} ' -> {\"Greg\": 8, \"Sam\": -10}\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    '{\"Greg\": 8, \"Sam\": -10}',\n",
    "    \"{'Greg': 8, 'Sam': -10}\",\n",
    "    \"\"\"{'Greg': 8, 'Sam': -10}\"\"\",\n",
    "    \"\"\"{\"Greg\": 8, \"Sam\": -10}\"\"\",\n",
    "    \"\"\" {\"Greg\":  8, \"Sam\" : -10 }\"\"\",\n",
    "    '{\"Greg\": 8, \"Sam\": -10} ',\n",
    "]\n",
    "expected_output = {\"Greg\": 8, \"Sam\": -10}\n",
    "\n",
    "for i, case in enumerate(test_cases):\n",
    "    result = str_to_dict(case)\n",
    "    assert result == expected_output, f\"Test case {i+1} failed: expected {expected_output}, got {result}\"\n",
    "\n",
    "# print(\"All test cases passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make bias_dict_str an str\n",
    "# then make it json loads\n",
    "# good error handling\n",
    "# pretty docs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lenz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
